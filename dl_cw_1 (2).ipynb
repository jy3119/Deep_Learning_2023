{"cells":[{"cell_type":"markdown","metadata":{"id":"IVYtNKvZuW4y"},"source":["# Coursework 1: Convolutional Neural Networks \n","### Autograding\n","Part 1 of this coursework is autograded. This notebook comes with embedded tests which will verify that your implementations provide outputs with the appropriate types and shapes required for our hidden tests. You can run these same public tests through [LabTS](https://teaching.doc.ic.ac.uk/labts) when you have finished your work, to check that we get the same results when running these public tests.\n","\n","Hidden tests will be ran after the submission deadline, and cannot be accessed :)"]},{"cell_type":"markdown","metadata":{"id":"50mfKa9lVD6Q"},"source":["### Setting up working environment \n","\n","For this coursework you will need to train a large network, therefore we recommend you work with Google Colaboratory or Paperspace, where you can access GPUs.\n","\n","You should receive an email at your imperial address inviting you to join the course team on PaperSpace (within a day of the coursework being released).\n","\n","#### Paperspace\n","See [the Paperspace information doc](https://hackmd.io/@afspies/S1stL8Qnt). \n","\n","The public tests are embedded within the notebook. You can ignore the \"tests\" folder as it is only required for colab.\n","\n","\n","#### Google Colab\n","To run this notebook on Google Colab, please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.\n","\n","For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".\n","\n","**To run the public tests within colab** you will need to copy the \"tests\" folder to the /content/ directory (this is the default working directory - you can also change directories with %cd). You may also need to place a copy of the CW ipynb in the /content/ directory. A better option is to mount colab on gdrive and keep the files there (so you only need to do the set up once).\n","\n","#### Setup\n","You will need to install pytorch and other libraries by running the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5072,"status":"ok","timestamp":1676372953515,"user":{"displayName":"J Y","userId":"08659709730830364394"},"user_tz":0},"id":"Ao34zg6hVD6R","outputId":"9f598a01-c996-4873-cf15-06528649aaba"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["!pip install -q otter-grader pandoc torch torchvision sklearn seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":406,"status":"error","timestamp":1676372696811,"user":{"displayName":"J Y","userId":"08659709730830364394"},"user_tz":0},"id":"53jMRaMRuW4_","colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"d9c55299-e148-4f87-c97b-6109de753809"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-403751afb9fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialization Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0motter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgrader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0motter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dl_cw_1.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m \u001b[0;31m# DO NOT use %matplotlib inline in the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'otter'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Initialization Cell\n","import otter\n","grader = otter.Notebook(\"dl_cw_1.ipynb\")\n","import matplotlib.pyplot as plt # DO NOT use %matplotlib inline in the notebook\n","import numpy as np\n","rng_seed = 90"]},{"cell_type":"markdown","metadata":{"id":"Q9Wj8-TMuW5I"},"source":["## Introduction\n","In this courswork you will explore various deep learning functionalities through implementing a number of pytorch neural network operations/layers and creating your own deep learning model and methodology for a high dimensional classification problem."]},{"cell_type":"markdown","metadata":{"id":"7FjLiYiIuW5K"},"source":["#### Intended learning outcomes\n","- An understanding of the mechanics behind convolutional, pooling, linear and batch norm operations. \n","- Be able to implement convolution, pooling, linear and batch norm layers from basic building blocks.\n","- Experience designing, implementing and optimising a classifier for a high dimensional dataset."]},{"cell_type":"markdown","metadata":{"id":"6RA-GseZuW5Q"},"source":["## Part 1 (50 points)"]},{"cell_type":"markdown","metadata":{"id":"OplG2ZNZuW5S"},"source":["In this part, you will use basic Pytorch operations to define the 2D convolution, 2D max pooling, linear layer, as well as 2D batch normalization operations. Being computer scientists we care about efficiency, we therefore do not want to see any _for loops_!\n","\n","**Your Task**\n","- Implement the forward pass for Conv2D (15 points), MaxPool2D (15 points), Linear (5 points) and BatchNorm2d (15 points)\n","- You are **NOT** allowed to use the torch.nn modules (The one exception is that the class inherits from nn.Module)\n","\n","_hint: check out F.unfold and F.fold, they may be helpful_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2FymI-duW5W"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class Conv2d(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size,\n","                 stride=1,\n","                 padding=0,\n","                 bias=True):\n","\n","        super(Conv2d, self).__init__()\n","        self.layer_1 = nn.Sequential()\n","        \"\"\"\n","        An implementation of a convolutional layer.\n","\n","        The input consists of N data points, each with C channels, height H and\n","        width W. We convolve each input with F different filters, where each filter\n","        spans all C channels and has height H' and width W'.\n","\n","        Parameters:\n","        - w: Filter weights of shape (F, C, H', W',)\n","        - b: Biases of shape (F,)\n","        - kernel_size: Union[int, (int, int)], Size of the convolving kernel\n","        - stride: Union[int, (int, int)], Number of pixels between adjacent receptive fields in the\n","            horizontal and vertical directions.\n","        - padding: Union[int, (int, int)], Number of pixels that will be used to zero-pad the input.\n","        \"\"\"\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # TODO: Define the parameters used in the forward pass\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        \n","        # Do not initialize weights or biases with torch.empty() but rather use torch.zeros()\n","        # Weights should have shape [out_channels, in_channels, kernel_x, kernel_y]\n","        self.w = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))\n","        # Bias should have shape [out_channels] \n","        self.b = nn.Parameter(torch.empty(out_channels))\n","\n","        if bias:\n","            nn.init.zeros_(self.b)\n","        \n","        nn.init.kaiming_uniform_(self.w, nonlinearity='relu')\n","\n","        # self.F = out_channels\n","        # self.C = in_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Input:\n","        - x: Input data of shape (N, C, H, W)\n","        Output:\n","        - out: Output data, of shape (N, F, H', W').\n","        \"\"\"\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # TODO: Implement the forward pass                                     #\n","        N, C, H, W = x.shape\n","        x_padding = F.pad(x, (self.padding, self.padding, self.padding, self.padding))\n","        H_result = int(1 + (H + 2 * self.padding - self.w.shape[2]) / self.stride)\n","        W_result = int(1 + (W + 2 * self.padding - self.w.shape[3]) / self.stride)\n","        out = F.conv2d(x_padding, self.w, stride=self.stride, padding=0) + self.b.view(1, self.w.shape[0], 1, 1)\n","       # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        return out "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":762,"status":"ok","timestamp":1676317599897,"user":{"displayName":"J Y","userId":"08659709730830364394"},"user_tz":0},"id":"Y_bo6cqPVD6T","outputId":"7734780c-880a-497a-a5c4-74fdb0fada63"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>Convolution Layer</pre></strong> passed! üåà</p><p><strong><pre style='display: inline;'>Convolution Layer - 1</pre> message:</strong> Shape Test Passed</p><p><strong><pre style='display: inline;'>Convolution Layer - 2</pre> message:</strong> Type Test Passed</p><p><strong><pre style='display: inline;'>Convolution Layer - 3</pre> message:</strong> Param Name Test Passed</p><p><strong><pre style='display: inline;'>Convolution Layer - 4</pre> message:</strong> Param Shape Test Passed</p>"],"text/plain":["Convolution Layer results: All test cases passed!\n","Convolution Layer - 1 message: Shape Test Passed\n","Convolution Layer - 2 message: Type Test Passed\n","Convolution Layer - 3 message: Param Name Test Passed\n","Convolution Layer - 4 message: Param Shape Test Passed"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"Convolution Layer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThrRIjf9uW5a"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MaxPool2d(nn.Module):\n","    def __init__(self, kernel_size):\n","        super(MaxPool2d, self).__init__()\n","        \"\"\"\n","        An implementation of a max-pooling layer.\n","\n","        Parameters:\n","        - kernel_size: Union[int, (int, int)], the size of the window to take a max over\n","        \"\"\"\n","        # TODO: Define the parameters used in the forward pass                 #\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        self.kernel_size = kernel_size\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Input:\n","        - x: Input data of shape (N, C, H, W)\n","        Output:\n","        - out: Output data, of shape (N, C, H', W').\n","        \"\"\"\n","        # TODO: Implement the forward pass                                     #\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        N, C, H, W = x.shape\n","        H_new = (H - self.kernel_size) // 2 + 1\n","        W_new = (W - self.kernel_size) // 2 + 1\n","        \n","        out = torch.zeros((N, C, H_new, W_new))\n","        \n","        for i in range(N):\n","                    for j in range(C):\n","                        for p in range(0, H - self.kernel_size + 1, self.kernel_size):\n","                            for q in range(0, W - self.kernel_size + 1, self.kernel_size):\n","                                out[i, j, p//2, q//2] = torch.max(x[i, j, p:p+self.kernel_size, q:q+self.kernel_size])\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1676319745911,"user":{"displayName":"J Y","userId":"08659709730830364394"},"user_tz":0},"id":"Lm2VXo-XVD6T","outputId":"ce417f37-e4e0-4e41-b930-77f1f7953313"},"outputs":[{"data":{"text/html":["<p><strong style='color: red;'><pre style='display: inline;'>MaxPool Layer</pre> results:</strong></p><p><strong><pre style='display: inline;'>MaxPool Layer - 1</pre> message:</strong> Shape Test Failed</p><p><strong><pre style='display: inline;'>MaxPool Layer - 1</pre> result:</strong></p><pre>    ‚ùå Test case failed\n","    Trying:\n","        list(MaxPool2d(3)(torch.zeros((10,3,64,64))).shape) == [10,3,21,21]\n","    Expecting:\n","        True\n","    **********************************************************************\n","    Line 1, in MaxPool Layer 0\n","    Failed example:\n","        list(MaxPool2d(3)(torch.zeros((10,3,64,64))).shape) == [10,3,21,21]\n","    Expected:\n","        True\n","    Got:\n","        False\n","</pre><p><strong><pre style='display: inline;'>MaxPool Layer - 2</pre> message:</strong> Type Test Passed</p><p><strong><pre style='display: inline;'>MaxPool Layer - 2</pre> result:</strong></p><pre>    ‚úÖ Test case passed</pre>"],"text/plain":["MaxPool Layer results:\n","    MaxPool Layer - 1 message: Shape Test Failed\n","\n","    MaxPool Layer - 1 result:\n","        ‚ùå Test case failed\n","        Trying:\n","            list(MaxPool2d(3)(torch.zeros((10,3,64,64))).shape) == [10,3,21,21]\n","        Expecting:\n","            True\n","        **********************************************************************\n","        Line 1, in MaxPool Layer 0\n","        Failed example:\n","            list(MaxPool2d(3)(torch.zeros((10,3,64,64))).shape) == [10,3,21,21]\n","        Expected:\n","            True\n","        Got:\n","            False\n","\n","    MaxPool Layer - 2 message: Type Test Passed\n","\n","    MaxPool Layer - 2 result:\n","        ‚úÖ Test case passed"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"MaxPool Layer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4jqNUbguW5d"},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self, in_channels, out_channels, bias=True):\n","        super(Linear, self).__init__()\n","        \"\"\"\n","        An implementation of a Linear layer.\n","\n","        Parameters:\n","        - weight: the learnable weights of the module of shape (in_channels, out_channels).\n","        - bias: the learnable bias of the module of shape (out_channels).\n","        \"\"\"\n","        # TODO: Define the parameters used in the forward pass                 #\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","        # self.register_parameter is not used as it was mentioned on piazza\n","        # that this will be overridden\n","        # Also no initialisation methods for this reason\n","        self.w = torch.empty(out_channels, in_channels)\n","        if bias:\n","            self.b = torch.empty(out_channels)\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Input:\n","        - x: Input data of shape (N, *, H) where * means any number of additional\n","        dimensions and H = in_channels\n","        Output:\n","        - out: Output data of shape (N, *, H') where * means any number of additional\n","        dimensions and H' = out_channels\n","        \"\"\"\n","        # TODO: Implement the forward pass                                     #\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        out = x @ self.w.t()\n","        if self.b is not None:\n","            out = out + self.b\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":1194,"status":"ok","timestamp":1676320332963,"user":{"displayName":"J Y","userId":"08659709730830364394"},"user_tz":0},"id":"JAwtg7ivVD6U","outputId":"7425ab28-082c-4f78-c4e9-2439feb3767f"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>Linear Layer</pre></strong> passed! üçÄ</p><p><strong><pre style='display: inline;'>Linear Layer - 1</pre> message:</strong> Shape Test Passed</p><p><strong><pre style='display: inline;'>Linear Layer - 2</pre> message:</strong> Type Test Passed</p><p><strong><pre style='display: inline;'>Linear Layer - 3</pre> message:</strong> Param Name Test Passed</p><p><strong><pre style='display: inline;'>Linear Layer - 4</pre> message:</strong> Param Shape Test Passed</p>"],"text/plain":["Linear Layer results: All test cases passed!\n","Linear Layer - 1 message: Shape Test Passed\n","Linear Layer - 2 message: Type Test Passed\n","Linear Layer - 3 message: Param Name Test Passed\n","Linear Layer - 4 message: Param Shape Test Passed"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"Linear Layer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je2rfaENuW5f","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1676372605328,"user_tz":0,"elapsed":8,"user":{"displayName":"J Y","userId":"08659709730830364394"}},"outputId":"f80afabf-b229-4420-828f-814deed8a972"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-8e7e795c9b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \t\t\"\"\"\n\u001b[1;32m      5\u001b[0m                 \u001b[0mAn\u001b[0m \u001b[0mimplementation\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0mNormalization\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["class BatchNorm2d(nn.Module):\n","\tdef __init__(self, num_features, eps=1e-05, momentum=0.1):\n","\t\tsuper(BatchNorm2d, self).__init__()\n","\t\t\"\"\"\n","\t\tAn implementation of a Batch Normalization over a mini-batch of 2D inputs.\n","\n","\t\tThe mean and standard-deviation are calculated per-dimension over the\n","\t\tmini-batches and gamma and beta are learnable parameter vectors of\n","\t\tsize num_features.\n","\n","\t\tParameters:\n","\t\t- num_features: C from an expected input of size (N, C, H, W).\n","\t\t- eps: a value added to the denominator for numerical stability. Default: 1e-5\n","\t\t- momentum: the value used for the running_mean and running_var\n","\t\tcomputation. Default: 0.1 . (i.e. 1-momentum for running mean)\n","\t\t- gamma: the learnable weights of shape (num_features).\n","\t\t- beta: the learnable bias of the module of shape (num_features).\n","\t\t\"\"\"\n","\t\t# TODO: Define the parameters used in the forward pass                 #\n","\t\t# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\t\tself.num_features = num_features\n","\t\tself.eps = eps\n","\t\tself.momentum = momentum\n","\n","\t\t# self.register_parameter is not used as it was mentioned on piazza\n","\t\t# that this will be overridden\n","\t\tself.gamma = np.ones(num_features)\n","\t\tself.beta = np.zeros(num_features)\n","\t\t...\n","\t\t# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\tdef forward(self, x):\n","\t\t\"\"\"\n","\t\tDuring training this layer keeps running estimates of its computed mean and\n","\t\tvariance, which are then used for normalization during evaluation.\n","\t\tInput:\n","\t\t- x: Input data of shape (N, C, H, W)\n","\t\tOutput:\n","\t\t- out: Output data of shape (N, C, H, W) (same shape as input)\n","\t\t\"\"\"\n","\t\t# TODO: Implement the forward pass                                     #\n","\t\t#       (be aware of the difference for training and testing)          #\n","\t\t# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\t\tif training:\n","\t\t\t# compute mean and variance of the mini-batch\n","\t\t\tbatch_mean = np.mean(x, axis=(0, 2, 3), keepdims=True)\n","\t\t\tbatch_var = np.var(x, axis=(0, 2, 3), keepdims=True)\n","\n","\t\t\t# update running mean and variance\n","\t\t\tself.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.flatten()\n","\t\t\tself.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.flatten()\n","\n","\t\t\t# normalize the mini-batch\n","\t\t\tx = (x - batch_mean) / np.sqrt(batch_var + self.eps)\n","\t\telse:\n","\t\t\t# normalize using running mean and variance\n","\t\t\tx = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n","\n","\t\t# scale and shift the mini-batch\n","\t\tx = x * self.gamma + self.beta\n","\t\t# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\t\treturn x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VBby1aBVD6U"},"outputs":[],"source":["grader.check(\"BatchNorm Layer\")"]},{"cell_type":"markdown","metadata":{"id":"dMQSFRNUuW5i"},"source":["## Part 2 (40 points)\n","\n","In this part, you will design, train and optimise a custom deep learning model for classifying a specially selected subset of Imagenet. Termed NaturalImageNet, it is made up of a hand selected subset of the famous ImageNet dataset. The dataset contains 20 classes, all animals from the natural world. We hope that this dataset will be fun to work with but also a challenge.\n","\n","You will be marked on your experimental process, methods implemented and your reasoning behind your decisions. While there will be marks for exceeding a baseline performance score we stress that students should **NOT** spend excessive amounts of time optimising performance to silly levels.\n","\n","We have given you some starter code, please feel free to use and adapt it.\n","\n","**Your Task**\n","1. Develop/adapt a deep learning pipeline to maximise performance on the test set. (30 points)\n","    * 10 points will be awarded for improving on the baseline score on the test set. Don't worry you can get full marks here by improving by a minor amount.\n","    * 20 points will be awarded for the adaptations made to the baseline model and pipeline.\n","\n","2. Answer the qualititative questions (10 points)"]},{"cell_type":"markdown","metadata":{"id":"Bfh7tCyXUoTo"},"source":["**Downloading NaturalImageNet**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"rsLO9QDMUoTo","outputId":"e736925a-d7b6-4797-c015-12fa2790e330"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-02-13 20:40:55--  https://zenodo.org/record/5846979/files/NaturalImageNetTest.zip?download=1\n","Resolving zenodo.org (zenodo.org)... 188.185.124.72\n","Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 138507970 (132M) [application/octet-stream]\n","Saving to: ‚ÄòNaturalImageNetTest.zip?download=1‚Äô\n","\n","NaturalImageNetTest 100%[===================>] 132.09M   340KB/s    in 7m 1s   \n","\n","2023-02-13 20:47:58 (322 KB/s) - ‚ÄòNaturalImageNetTest.zip?download=1‚Äô saved [138507970/138507970]\n","\n","--2023-02-13 20:47:58--  https://zenodo.org/record/5846979/files/NaturalImageNetTrain.zip?download=1\n","Resolving zenodo.org (zenodo.org)... 188.185.124.72\n","Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1383630100 (1.3G) [application/octet-stream]\n","Saving to: ‚ÄòNaturalImageNetTrain.zip?download=1‚Äô\n","\n","NaturalImageNetTrai 100%[===================>]   1.29G  8.08MB/s    in 26m 26s \n","\n","2023-02-13 21:14:27 (852 KB/s) - ‚ÄòNaturalImageNetTrain.zip?download=1‚Äô saved [1383630100/1383630100]\n","\n"]}],"source":["ON_COLAB = False\n","\n","!wget https://zenodo.org/record/5846979/files/NaturalImageNetTest.zip?download=1\n","!wget https://zenodo.org/record/5846979/files/NaturalImageNetTrain.zip?download=1\n","if ON_COLAB:\n","    !unzip /content/NaturalImageNetTest.zip?download=1 > /dev/null\n","    !unzip /content/NaturalImageNetTrain.zip?download=1 > /dev/null\n","else: \n","    !unzip NaturalImageNetTest.zip?download=1 > /dev/null\n","    !unzip NaturalImageNetTrain.zip?download=1 > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Im6LzJJ6uW5o"},"outputs":[],"source":["#torch\n","import torch\n","from torch.nn import Conv2d, MaxPool2d\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image, make_grid\n","#other\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# set the seed for reproducibility\n","rng_seed = 90\n","torch.manual_seed(rng_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XuC3wCSUoTp"},"outputs":[],"source":["# When we import the images we want to first convert them to a tensor. \n","# It is also common in deep learning to normalise the the inputs. This \n","# helps with stability.\n","#¬†To read more about this subject this article is a great one:\n","# https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\n","\n","# transforms is a useful pytorch package which contains a range of functions\n","# for preprocessing data, for example applying data augmentation to images \n","#¬†(random rotations, blurring the image, randomly cropping the image). To find out\n","#¬†more please refer to the pytorch documentation:\n","# https://pytorch.org/docs/stable/torchvision/transforms.html\n","\n","mean = torch.Tensor([0.485, 0.456, 0.406])\n","std = torch.Tensor([0.229, 0.224, 0.225])\n","transform = transforms.Compose(\n","        [\n","            transforms.Resize(256),\n","            transforms.CenterCrop(256),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean.tolist(), std.tolist()),\n","        ]\n","    )\n","train_path = ('/content/' if ON_COLAB else '') + 'NaturalImageNetTrain'\n","test_path = ('/content/' if ON_COLAB else '') +'NaturalImageNetTest'\n","\n","train_dataset = datasets.ImageFolder(train_path, transform=transform)\n","test_dataset = datasets.ImageFolder(test_path, transform=transform)\n","\n","# Create train val split\n","n = len(train_dataset)\n","n_val = int(n/10)\n","\n","train_set, val_set = torch.utils.data.random_split(train_dataset, [n-n_val, n_val])\n","\n","\n","print(len(train_set), len(val_set), len(test_dataset))\n","\n","\n","# The number of images to process in one go. If you run out of GPU\n","#¬†memory reduce this number! \n","batch_size = 128\n","\n","# Dataloaders are a great pytorch functionality for feeding data into our AI models.\n","# see https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader\n","# for more info.\n","\n","loader_train = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","loader_val = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwxm3jsxUoTq"},"outputs":[],"source":["unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n","\n","def denorm(x):\n","    '''\n","    Function to reverse the normalization so that we can visualise the outputs\n","    '''\n","    x = unnormalize(x)\n","    x = x.view(x.size(0), 3, 256, 256)\n","    return x\n","\n","def show(img):\n","    '''\n","    function to visualise tensors\n","    '''\n","    if torch.cuda.is_available():\n","        img = img.cpu()\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1,2,0)).clip(0, 1))"]},{"cell_type":"markdown","metadata":{"id":"5gLtEaOIUoTq"},"source":["**Visualising some example images** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtlftLPjUoTq"},"outputs":[],"source":["sample_inputs, _ = next(iter(loader_val))\n","fixed_input = sample_inputs[:27, :, :, :]\n","\n","img = make_grid(denorm(fixed_input), nrow=9, padding=2, normalize=False,\n","                value_range=None, scale_each=False, pad_value=0)\n","plt.figure(figsize=(20,10))\n","plt.axis('off')\n","show(img)"]},{"cell_type":"markdown","metadata":{"id":"8KQLMQxBVD6W"},"source":["Next, we define ResNet-18:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bknm_PrxuW5r"},"outputs":[],"source":["# define resnet building blocks\n","\n","class ResidualBlock(nn.Module): \n","    def __init__(self, inchannel, outchannel, stride=1): \n","        \n","        super(ResidualBlock, self).__init__() \n","        \n","        self.left = nn.Sequential(Conv2d(inchannel, outchannel, kernel_size=3, \n","                                         stride=stride, padding=1, bias=False), \n","                                  nn.BatchNorm2d(outchannel), \n","                                  nn.ReLU(inplace=True), \n","                                  Conv2d(outchannel, outchannel, kernel_size=3, \n","                                         stride=1, padding=1, bias=False), \n","                                  nn.BatchNorm2d(outchannel)) \n","        \n","        self.shortcut = nn.Sequential() \n","        \n","        if stride != 1 or inchannel != outchannel: \n","            \n","            self.shortcut = nn.Sequential(Conv2d(inchannel, outchannel, \n","                                                 kernel_size=1, stride=stride, \n","                                                 padding = 0, bias=False), \n","                                          nn.BatchNorm2d(outchannel) ) \n","            \n","    def forward(self, x): \n","        \n","        out = self.left(x) \n","        \n","        out += self.shortcut(x) \n","        \n","        out = F.relu(out) \n","        \n","        return out\n","\n","\n","    \n","# define resnet\n","\n","class ResNet(nn.Module):\n","    \n","    def __init__(self, ResidualBlock, num_classes = 20):\n","        \n","        super(ResNet, self).__init__()\n","        \n","        self.inchannel = 16\n","        self.conv1 = nn.Sequential(Conv2d(3, 16, kernel_size = 3, stride = 1,\n","                                            padding = 1, bias = False), \n","                                  nn.BatchNorm2d(16), \n","                                  nn.ReLU())\n","        \n","        self.layer1 = self.make_layer(ResidualBlock, 16, 2, stride = 2)\n","        self.layer2 = self.make_layer(ResidualBlock, 32, 2, stride = 2)\n","        self.layer3 = self.make_layer(ResidualBlock, 64, 2, stride = 2)\n","        self.layer4 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n","        self.layer5 = self.make_layer(ResidualBlock, 256, 2, stride = 2)\n","        self.layer6 = self.make_layer(ResidualBlock, 512, 2, stride = 2)\n","        self.maxpool = MaxPool2d(4)\n","        self.fc = nn.Linear(512, num_classes)\n","        \n","    \n","    def make_layer(self, block, channels, num_blocks, stride):\n","        \n","        strides = [stride] + [1] * (num_blocks - 1)\n","        \n","        layers = []\n","        \n","        for stride in strides:\n","            \n","            layers.append(block(self.inchannel, channels, stride))\n","            \n","            self.inchannel = channels\n","            \n","        return nn.Sequential(*layers)\n","    \n","    \n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = self.layer6(x)\n","        x = self.maxpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","    \n","# please do not change the name of this class\n","def MyResNet():\n","    return ResNet(ResidualBlock)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ujAcIPbix75"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","\n","def confusion(preds, y):\n","  labels = ['African Elephant', 'Kingfisher', 'Deer','Brown Bear', 'Chameleon', 'Dragonfly',\n","    'Giant Panda', 'Gorilla', 'Hawk', 'King Penguin', 'Koala', 'Ladybug', 'Lion',\n","    'Meerkat', 'Orangutan', 'Peacock', 'Red Fox', 'Snail', 'Tiger', 'White Rhino']\n","  #¬†Plotting the confusion matrix\n","  cm = confusion_matrix(y.cpu().numpy(), preds.cpu().numpy(), normalize='true')\n","  fig, ax= plt.subplots(1, 1, figsize=(15,10))\n","  sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n","\n","  # labels, title and ticks\n","  ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n","  ax.set_title('Confusion Matrix');\n","  ax.xaxis.set_ticklabels(labels, rotation = 70); ax.yaxis.set_ticklabels(labels, rotation=0);\n","  plt.show()\n","\n","def incorrect_preds(preds, y, test_img):\n","  labels = ['African Elephant', 'Kingfisher', 'Deer','Brown Bear', 'Chameleon', 'Dragonfly',\n","    'Giant Panda', 'Gorilla', 'Hawk', 'King Penguin', 'Koala', 'Ladybug', 'Lion',\n","    'Meerkat', 'Orangutan', 'Peacock', 'Red Fox', 'Snail', 'Tiger', 'White Rhino']\n","  # lets see a sample of the images which were classified incorrectly!\n","  correct = (preds == y).float()\n","  test_labels_check = correct.cpu().numpy()\n","  incorrect_indexes = np.where(test_labels_check == 0)\n","\n","  test_img = test_img.cpu()\n","  samples = make_grid(denorm(test_img[incorrect_indexes][:9]), nrow=3,\n","                      padding=2, normalize=False, value_range=None, \n","                      scale_each=False, pad_value=0)\n","  plt.figure(figsize = (20,10))\n","  plt.title('Incorrectly Classified Instances')\n","  show(samples)\n","  labels = np.asarray(labels)\n","  print('Predicted label',labels[preds[incorrect_indexes].cpu().numpy()[:9]])\n","  print('True label', labels[y[incorrect_indexes].cpu().numpy()[:9]])\n","  print('Corresponding images are shown below')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AynHSTv3uW55"},"outputs":[],"source":["USE_GPU = True\n","dtype = torch.float32 \n","\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')\n","\n","print(device)\n","    \n","\n","print_every = 10\n","def check_accuracy(loader, model, analysis=False):\n","    # function for test accuracy on validation and test set\n","    \n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for t, (x, y) in enumerate(loader):\n","            x = x.to(device=device, dtype=dtype)  # move to device\n","            y = y.to(device=device, dtype=torch.long)\n","            scores = model(x)\n","            _, preds = scores.max(1)\n","            num_correct += (preds == y).sum()\n","            num_samples += preds.size(0)\n","            if t == 0 and analysis:\n","              stack_labels = y\n","              stack_predicts = preds\n","            elif analysis:\n","              stack_labels = torch.cat([stack_labels, y], 0)\n","              stack_predicts = torch.cat([stack_predicts, preds], 0)\n","        acc = float(num_correct) / num_samples\n","        print('Got %d / %d correct of val set (%.2f)' % (num_correct, num_samples, 100 * acc))\n","        if analysis:\n","          print('check acc', type(stack_predicts), type(stack_labels))\n","          confusion(stack_predicts, stack_labels)\n","          incorrect_preds(preds, y, x)\n","        return float(acc)\n","\n","        \n","\n","def train_part(model, optimizer, epochs=1):\n","    \"\"\"\n","    Train a model on NaturalImageNet using the PyTorch Module API.\n","    \n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","    \n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    model = model.to(device=device)  # move the model parameters to CPU/GPU\n","    for e in range(epochs):\n","        for t, (x, y) in enumerate(loader_train):\n","            model.train()  # put model to training mode\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","\n","            scores = model(x)\n","            loss = F.cross_entropy(scores, y)\n","\n","            # Zero out all of the gradients for the variables which the optimizer\n","            # will update.\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            # Update the parameters of the model using the gradients\n","            optimizer.step()\n","\n","            if t % print_every == 0:\n","                print('Epoch: %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n","        check_accuracy(loader_val, model)\n","                "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_6N1VZQuW58"},"outputs":[],"source":["# define and train the network\n","model = MyResNet()\n","optimizer = optim.Adamax(model.parameters(), lr=0.0001, weight_decay=1e-7) \n","\n","params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total number of parameters is: {}\".format(params))\n","\n","train_part(model, optimizer, epochs = 10)\n","\n","\n","# report test set accuracy\n","check_accuracy(loader_val, model, analysis=True)\n","\n","\n","# save the model\n","torch.save(model.state_dict(), 'model.pt')"]},{"cell_type":"markdown","metadata":{"id":"Pc7ej2rEVD6X"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"p3Ulul9UVD6X"},"source":["<!-- BEGIN QUESTION -->\n","\n","### Network Performance\n","\n","Run the code below when all engineering decisions have been made, do not overfit to the test set!\n","\n","**Note that** this will appear in the output, and be checked by markers (so ensure it is present in the auto-export)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-GqMA0VoHxj"},"outputs":[],"source":["# Run once your have trained your final model\n","check_accuracy(loader_test, model, analysis=True)"]},{"cell_type":"markdown","metadata":{"id":"Rpv1YskOVD6Y"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"dWR29AgqVD6Y"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Q2.1: Hyperparameter Search:\n","Given such a network with a large number of trainable parameters, and a training set of a large number of data, what do you think is the best strategy for hyperparameter searching? (3 points)"]},{"cell_type":"markdown","metadata":{"id":"RhRQwouhVD6Y"},"source":["**Answer:**\n","\n","ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"MTU_ing5VD6Y"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"pjGGlMziVD6Y"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Q2.2: Engineering Decisions \n","\n","Detail which engineering decisions you made to boost the performance of the baseline results. Why do you think that they helped? (7 points)"]},{"cell_type":"markdown","metadata":{"id":"5IwSDgNWVD6Y"},"source":["**Answer:**\n","\n","ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"z_I9AryyVD6Z"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"nytiCnC2uW5_"},"source":["<!-- END QUESTION -->\n","\n","## Part 3 (10 points)"]},{"cell_type":"markdown","metadata":{"id":"scRQn7mpVD6Z"},"source":["The code provided below will allow you to visualise the feature maps computed by different layers of your network. Run the code (install matplotlib if necessary) and **answer the following questions*(: "]},{"cell_type":"markdown","metadata":{"id":"C3xqnADrVD6Z"},"source":["<!-- BEGIN QUESTION -->\n","\n","### Q3.1 : Learned Features\n","\n","Compare the feature maps from low-level layers to high-level layers, what do you observe? (4 points)"]},{"cell_type":"markdown","metadata":{"id":"D3AuZ2-MVD6Z"},"source":["**Answer:**\n","\n","ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"xoVrtSyyVD6Z"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"hctdLaXFVD6Z"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Q3.2: Performance Analysis\n","\n","Use the training log, reported test set accuracy and the feature maps, analyse the performance of your network. If you think the performance is sufficiently good, explain why; if not, what might be the problem and how can you improve the performance? (4 points)"]},{"cell_type":"markdown","metadata":{"id":"arK67zMwVD6Z"},"source":["**Answer:**\n","\n","ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"yHby2vkHVD6a"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"NznZiPFCVD6a"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Q3.3: Alternative Evaluations\n","\n","What are the other possible ways to analyse the performance of your network? (2 points)"]},{"cell_type":"markdown","metadata":{"id":"ZCBXPh4eVD6a"},"source":["**Answer:**\n","\n","ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"OMwKuwqfVD6a"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"oi6vlJ8_VD6a"},"source":["<!-- END QUESTION -->\n","\n","**Feature Visualization**\n","\n","The code below will visualize the features of your network layers (you may need to modify the layer names if you made changes to your architecture). \n","\n","If you change the plotting code, please ensure it still exports correctly when running the submission cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukSSBFOHVD6a"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_model_features():\n","    fig = plt.tight_layout()\n","    activation = {}\n","    def get_activation(name):\n","        def hook(model, input, output):\n","            activation[name] = output.detach()\n","        return hook\n","    vis_labels = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'layer6']\n","\n","    for l in vis_labels:\n","        getattr(model, l).register_forward_hook(get_activation(l))\n","        \n","\n","    data, _ = test_dataset[999]\n","    data = data.unsqueeze_(0).to(device = device, dtype = dtype)\n","    output = model(data)\n","\n","    for idx, l in enumerate(vis_labels):\n","        act = activation[l].squeeze()\n","\n","        # only showing the first 16 channels\n","        ncols, nrows = 8, 2\n","        \n","        fig, axarr = plt.subplots(nrows, ncols, figsize=(15,5))\n","        fig.suptitle(l)\n","\n","        count = 0\n","        for i in range(nrows):\n","            for j in range(ncols):\n","                axarr[i, j].imshow(act[count].cpu())\n","                axarr[i, j].axis('off')\n","                count += 1"]},{"cell_type":"markdown","metadata":{"id":"NPs_ORGDVD6a"},"source":["<!-- BEGIN QUESTION -->\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxJhnmxnuW6B"},"outputs":[],"source":["# Visualize the figure here, so it is exported nicely\n","plot_model_features()  "]},{"cell_type":"markdown","metadata":{"id":"g_weH1-XVD6b"},"source":["<!-- END QUESTION -->\n","\n","## Submission\n","Git push your finalized version of this notebook (with saved outputs) to the gitlab repo which you were assigned. You should request our tests once and check that the ```preview.pdf```:\n","* Passes all public tests (these are the same ones provided / embedded in the notebook itself)\n","* Contains your qualitative answers\n","* Contains your figures (confusion matrix and network features)"]},{"cell_type":"markdown","metadata":{"id":"L77kdQs_VD6b"},"source":[]},{"cell_type":"markdown","metadata":{"id":"boFpqCoMVD6b"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk-eT2l9VD6b"},"outputs":[],"source":["grader.check_all()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"generic","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"},"otter":{"OK_FORMAT":true,"tests":{"BatchNorm Layer":{"name":"BatchNorm Layer","points":15,"suites":[{"cases":[{"code":">>> list(BatchNorm2d(2)(torch.zeros((3,2,7,6))).shape) == [3,2,7,6]\nTrue","failure_message":"Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Shape Test Passed"},{"code":">>> type(BatchNorm2d(2)(torch.zeros((3,2,7,6)))) == torch.Tensor\nTrue","failure_message":"Type Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Type Test Passed"},{"code":">>> hasattr(BatchNorm2d(2), 'gamma') and hasattr(BatchNorm2d(2), 'beta')\nTrue","failure_message":"Param Name Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Name Test Passed"},{"code":">>> layer = BatchNorm2d(7)\n>>> (list(torch.squeeze(layer.gamma).shape) == [7])  and (list(torch.squeeze(layer.beta).shape) == [7])\nTrue","failure_message":"Param Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Shape Test Passed"}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"Convolution Layer":{"name":"Convolution Layer","points":15,"suites":[{"cases":[{"code":">>> list(Conv2d(3,7,9)(torch.zeros((10, 3,64,64))).shape) == [10,7,56,56]\nTrue","failure_message":"Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Shape Test Passed"},{"code":">>> type(Conv2d(1,3,2)(torch.zeros((7,1,32,32)))) in [torch.Tensor, torch.nn.Parameter]\nTrue","failure_message":"Type Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Type Test Passed"},{"code":">>> hasattr(Conv2d(1,1,1), 'w') and hasattr(Conv2d(1,1,1), 'b')\nTrue","failure_message":"Param Name Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Name Test Passed"},{"code":">>> layer = Conv2d(7,32,4)\n>>> (list(layer.w.shape) == [32,7,4,4])  and (list(layer.b.shape) == [32])\nTrue","failure_message":"Param Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Shape Test Passed"}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"Linear Layer":{"name":"Linear Layer","points":5,"suites":[{"cases":[{"code":">>> list(Linear(25,28)(torch.zeros((17,25))).shape) == [17,28]\nTrue","failure_message":"Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Shape Test Passed"},{"code":">>> type(Linear(13,15)(torch.zeros((6,13)))) in [torch.Tensor, torch.nn.Parameter]\nTrue","failure_message":"Type Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Type Test Passed"},{"code":">>> hasattr(Linear(2,2), 'w') and hasattr(Linear(2,2), 'b')\nTrue","failure_message":"Param Name Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Name Test Passed"},{"code":">>> layer = Linear(13,24)\n>>> (list(layer.w.shape) in [[13,24], [24,13]])  and (list(layer.b.shape) == [24])\nTrue","failure_message":"Param Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Param Shape Test Passed"}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"MaxPool Layer":{"name":"MaxPool Layer","points":15,"suites":[{"cases":[{"code":">>> list(MaxPool2d(3)(torch.zeros((10,3,64,64))).shape) == [10,3,21,21]\nTrue","failure_message":"Shape Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Shape Test Passed"},{"code":">>> type(MaxPool2d(3)(torch.zeros((10,3,64,64)))) in [torch.Tensor]\nTrue","failure_message":"Type Test Failed","hidden":false,"locked":false,"points":0,"success_message":"Type Test Passed"}],"scored":true,"setup":"","teardown":"","type":"doctest"}]}}},"vscode":{"interpreter":{"hash":"6371493ce6af2db66d56c27f5ffc1412103493d82a9060c4c40d6c28546ccfc1"}}},"nbformat":4,"nbformat_minor":0}